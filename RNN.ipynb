{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda :  False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "\n",
    "# HERE IMPLEMENT CUDA\n",
    "# if gpu is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "#use_cuda = False\n",
    "print(\"use_cuda : \", use_cuda)\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToLongTensor():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, inp):\n",
    "        return (torch.LongTensor(var) for var in inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharLSTM(torch.nn.Module):\n",
    "    def __init__(self, input_size, embedding_len, hidden_size, output_size, n_layers=1):\n",
    "        super().__init__()\n",
    "        #store input parameters in the object so we can use them later on\n",
    "        self.input_size = input_size\n",
    "        self.embedding_len = embedding_len\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #required functions for model\n",
    "        self.encoder = torch.nn.Embedding(input_size, embedding_len)\n",
    "        self.rnn = torch.nn.LSTM(embedding_len, hidden_size, n_layers, batch_first=True)\n",
    "        self.decoder = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        embedding = self.encoder(x.view(-1)) #encode our input into a vector embedding\n",
    "        output, hidden = self.rnn(embedding.view(-1, 1, self.embedding_len), hidden) #calculate the output from our rnn based on our input and previous hidden state\n",
    "        output = self.decoder(output.view(-1, self.hidden_size)) #calculate our output based on output of rnn\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, x):\n",
    "        return (torch.zeros(self.n_layers, x.shape[0], self.hidden_size),\n",
    "                torch.zeros(self.n_layers, x.shape[0], self.hidden_size)) #initialize our hidden and cell state to a matrix of 0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToLongTensor():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, inp):\n",
    "        return (torch.LongTensor(var) for var in inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WriterRNN():\n",
    "    def __init__(self, txt_file_path='Data/lyrics.txt', chunk_size=100, batch_size=32, train_epochs=50, transform=None, lower_case=True, lr=0.001):\n",
    "        self.txt_file_path = txt_file_path\n",
    "        self.chunk_size = chunk_size\n",
    "        self.transform = transform\n",
    "        self.lower_case = lower_case\n",
    "        self.was_read = False\n",
    "        self.batch_size = batch_size\n",
    "        self.train_epochs = train_epochs\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.lr = lr\n",
    "            \n",
    "    def read(self, embedding_len, hidden_size):\n",
    "        self.embedding_len = embedding_len\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        with open(self.txt_file_path, 'r') as file: # Imports data\n",
    "            rawtxt = file.read()\n",
    "\n",
    "        if self.lower_case: rawtxt = rawtxt.lower() # Converts to lower case by default\n",
    "        \n",
    "        letters = set(rawtxt)                                                         # List of unique characters\n",
    "        self.nchars = len(letters)                                                    # No. of unique characters\n",
    "        self.num_to_let = dict(enumerate(letters))                                    # Dictionary Mapping\n",
    "        self.let_to_num = dict(zip(self.num_to_let.values(), self.num_to_let.keys())) # Reverse Mapping\n",
    "        \n",
    "        txt = [self.let_to_num[letter] for letter in list(rawtxt)]      # Convert list of characters to mapped numbers\n",
    "        self.X = np.array(txt)                                          # Covert to numpy array\n",
    "        self.was_read = True\n",
    "        self.RNN = CharLSTM(self.nchars, self.embedding_len, self.hidden_size, self.nchars)      # Creates RNN\n",
    "        self.optimiser = torch.optim.Adam(self.RNN.parameters(), lr=self.lr)                     # Creates an optimiser\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X) - self.chunk_size           # The number of datapoints we have based on the chunk size and X\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx: idx + self.chunk_size]         # Get the chunk at the particular index\n",
    "        y = self.X[idx + 1: idx + self.chunk_size + 1] # Get label as a shifted input \n",
    "        \n",
    "        if self.transform:                             # If given apply the transformation\n",
    "            x, y = self.transform((x, y))\n",
    "    \n",
    "        return x, y\n",
    "    \n",
    "    def optimise(self, train_loader, generated_string, epoch_loss, epoch):\n",
    "        for idx, (x, y) in enumerate(train_loader):\n",
    "            loss = 0                                     #cost for this batch\n",
    "            h = self.RNN.init_hidden(x)                  #initialize our hidden state to 0s\n",
    "            for i in range(self.chunk_size):             #sequentially input each character in the sequence for each batch and calculate loss\n",
    "                out, h = self.RNN.forward(x[:, i], h)    #calculate outputs based on input and previous hidden state\n",
    "                \n",
    "                _, outl = out.data.max(1)                #based on our output, what character id does our network assign the highest probability of being next? # This is a [batch_size] sized Tensor\n",
    "                    \n",
    "                letter = self.num_to_let[outl[0].item()] #what chatacter is predicted for the 0th batch item?\n",
    "                generated_string += letter               #add the predicted letter to our generated sequence\n",
    "                \n",
    "                loss += self.criterion(out, y[:, i])     #add the cost for this input to the cost for the current batch\n",
    "            \n",
    "            writer.add_scalar('Loss/Train', loss/chunk_size, epoch*len(train_loader) + idx)    # write loss to a graph\n",
    "            \n",
    "            self.optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimiser.step()\n",
    "        \n",
    "        epoch_loss += loss.item()         #add the cost of this sequence to the cost of this epoch\n",
    "        return generated_string, epoch_loss\n",
    "    \n",
    "    def train(self):\n",
    "        if not self.was_read:\n",
    "            print(\"THE WRITER NEEDS TO READ BEFORE TRAINING!\")\n",
    "            return\n",
    "        print(\"I am going to read now. {} goddamn times!\".format(self.train_epochs))\n",
    "        self.train_loader = DataLoader(self, batch_size = self.batch_size, shuffle=True)\n",
    "        for epoch in range(self.train_epochs):\n",
    "            epoch_loss = 0              # Stores the cost for each epoch\n",
    "            generated_string = ''\n",
    "            generated_string, epoch_loss = self.optimise(self.train_loader, generated_string, epoch_loss, epoch)\n",
    "            epoch_loss /= len(train_loader.dataset) #divide by the number of datapoinst in each epoch\n",
    "\n",
    "            print('Epoch ', epoch+1, ' Avg Loss: ', epoch_loss)\n",
    "            print('Generated text: ', generated_string[0:150], '\\n')\n",
    "        \n",
    "    def maparray(self, txt):\n",
    "        tmp = [self.let_to_num[letter] for letter in list(txt)]\n",
    "        txt = np.array(tmp) #convert to numpy array\n",
    "        return txt\n",
    "    \n",
    "    def write_new(self, prime_str='a', str_len=150, temperature=0.75):\n",
    "        generated_string = 'NEW CREATION: '\n",
    "\n",
    "        prime_str = self.maparray(prime_str)          # use the maparray function to map the string to its character ids\n",
    "        x = torch.LongTensor(prime_str).unsqueeze(0)  # convert to LongTensor and add dimension to make batch size 1\n",
    "        h = self.init_hidden(x)                       # initialize hidden state\n",
    "\n",
    "        for i in range(x.shape[1]-1):                 # for each input character except the last\n",
    "            out, h = self.forward(x[:, i], h)         # feed that character into the network (prime hidden state)\n",
    "\n",
    "        x = x[:, -1]                        #get the last letter\n",
    "        for i in range(str_len):            #for each character we want to generate\n",
    "            out, h = self.forward(x, h)     #feed in the last character \n",
    "\n",
    "            out_dist = out.view(-1).div(temperature).exp() #get the output and exponentiate\n",
    "            sample = torch.multinomial(out_dist, 1).item() #turn into torch multinomial distribution and sample\n",
    "            pred_char = self.num_to_let[sample]                 #convert the sampled number into the corresponding character\n",
    "\n",
    "            generated_string += pred_char   #add the character to the generated string\n",
    "\n",
    "            x = torch.LongTensor([sample])  #set the last letter equal to the newly generated character\n",
    "\n",
    "        print(generated_string)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper-params\n",
    "lr = 0.001\n",
    "train_epochs = 50\n",
    "batch_size = 32\n",
    "chunk_size = 100\n",
    "embedding_len = 400\n",
    "hidden_size = 128\n",
    "\n",
    "writer = SummaryWriter() # we will use this to show our models performance on a graph\n",
    "\n",
    "Commie = WriterRNN(transform=ToLongTensor())\n",
    "\n",
    "Commie.read(embedding_len, \n",
    "            hidden_size\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am going to read now. 50 goddamn times!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-e20c452fe89e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mCommie\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-77b1e87e3849>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m              \u001b[0;31m# Stores the cost for each epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mgenerated_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mgenerated_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerated_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#divide by the number of datapoinst in each epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-77b1e87e3849>\u001b[0m in \u001b[0;36moptimise\u001b[0;34m(self, train_loader, generated_string, epoch_loss, epoch)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Commie.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Commie.write_new()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python37664bit6dfd275cebcf45d7aeef39af44a353d8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
